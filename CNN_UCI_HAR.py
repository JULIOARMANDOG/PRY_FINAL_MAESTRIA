# -*- coding: utf-8 -*-
"""CNN_uci_har_Semana2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12zTT4OXWPLFsKrhtcrbqvPmEXAoAepkf

1. Selección y Justificación del tipo de arquitectura a usar.
  Se escoge CNN dado que en varios estudios de este Dataset se han encontrado buenos resultados con ella, es rapida y no requiere tanto poder computacional como transformers o RNN, y promete alta eficiencia. Y como el dataset es pequeño este tipo de modelos es recomendable.
"""

import zipfile
import os
import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow.keras.utils import Sequence
from tensorflow.keras.models import Sequential
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from tensorflow.keras.layers import Conv1D, ReLU, BatchNormalization, GlobalMaxPooling1D, Dense, Input
from tensorflow.keras import Model
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.losses import SparseCategoricalCrossentropy
from tensorflow.keras.callbacks import EarlyStopping
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns

zip_path='uci-har.zip'
with zipfile.ZipFile(zip_path,'r') as zipRef:
  zipRef.extractall('sample_data/')

#Clase para cargar validar que los datos no contengan
#valores nulos y blancos

class validateHumanActivityRecognitionDataSet():

  def __init__(self,dataSetInput):
    self.dataSetInput=dataSetInput


  def validateNulls(self):
    countNan=0

    for itemData in self.dataSetInput:
      has_nan=np.isnan(itemData).any()
      if has_nan:
        countNan=countNan+1


    if  countNan >0 :
     print("Existe presencia de valores nulos en el dataset de entrenamiento")
    else:
     print("No existen valores nulos en la data de entrenamiento")

  def validateBlanks(self):
    countBlank=0

    for itemData in self.dataSetInput:
      has_blank=(itemData=="").any()
      if has_blank:
        countBlank=countBlank+1


    if  countBlank >0 :
     print("Existe presencia de valores blancos en el dataset de entrenamiento")
    else:
     print("No existen valores blancos  en la data de entrenamiento")

  def plotInformation(self,X,y):
    # Aplanar cada muestra: (n_samples, 9, 128) → (n_samples, 1152)
    n_samples = X.shape[0]
    X_flat = X.reshape(n_samples, -1)

    # Aplicar PCA para reducir a 2 dimensiones
    pca = PCA(n_components=2)
    X_pca = pca.fit_transform(X_flat)

    # Graficar en 2D
    plt.figure(figsize=(8, 6))
    scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='tab10', s=10)
    plt.title("Muestras UCI HAR reducidas con PCA (2D)")
    plt.xlabel("Componente Principal 1")
    plt.ylabel("Componente Principal 2")
    plt.legend(*scatter.legend_elements(), title="Actividad")
    plt.grid(True)
    plt.tight_layout()
    plt.show()

#clase para covertir dataset a para CNN 1D
class ConvertirDataSetHAR(Sequence):
    def __init__(self, X, y, batch_size=32,validation_split=0.0):
        self.X = X
        self.y = y
        self.batch_size = batch_size
        self.n_samples, self.n_channels, self.seq_len = X.shape
        self.validation_split = validation_split
        #Normalizar los datos , covertir el array 3D de forma (numero_ejemplos, numero_canales, numero_paso_tiempo_por_muestra) a
        #una representacion 2D de forma (numero_ejemplos * numero_canales, numero_paso_tiempo_por_muestra).
        self.X=self.X.reshape(-1,self.seq_len)
        #cada señal de 128 puntos se normaliza a media 0 y desviacion estandar 1
        self.X = (self.X - self.X.mean(axis=1, keepdims=True)) / self.X.std(axis=1, keepdims=True)
        #se retorna a la estructura original del dataset 3D
        self.X = self.X.reshape(self.n_samples, self.n_channels, self.seq_len)

        # Transponer de (n_samples, 9, 128) → (n_samples, 128, 9)
        self.X = self.X.transpose(0, 2, 1)

        if validation_split > 0.0:
            split_index = int(self.n_samples * (1 - validation_split))
            self.X_train, self.X_val = self.X[:split_index], self.X[split_index:]
            self.y_train, self.y_val = self.y[:split_index], self.y[split_index:]
            self.indexes = np.arange(self.X_train.shape[0])  # Índices para el conjunto de entrenamiento
        else:
           # Índices de los ejemplos
           self.indexes = np.arange(self.X.shape[0])

    def __len__(self):
        # Número total de batches por época
        return int(np.floor(self.n_samples / self.batch_size))

    def __getitem__(self, idx):
        # Devuelve un batch
        if self.validation_split > 0.0:
            batch_indexes = self.indexes[idx * self.batch_size:(idx + 1) * self.batch_size]
            batch_x = self.X_train[batch_indexes]
            batch_y = self.y_train[batch_indexes]
        else:
            batch_indexes = self.indexes[idx * self.batch_size:(idx + 1) * self.batch_size]
            batch_x = self.X[batch_indexes]
            batch_y = self.y[batch_indexes]
        return batch_x, batch_y

    def get_validation_data(self):
        # Devuelve los datos de validación si validation_split > 0.0
        if self.validation_split > 0.0:
            return self.X_val, self.y_val
        else:
            return None, None

def cnn_1d_model(input_shape, num_classes):
    # Definir la entrada
    inputs = tf.keras.Input(shape=input_shape)

    # Primera capa convolucional y de pooling
    x = tf.keras.layers.Conv1D(filters=32, kernel_size=3, activation="relu", strides=1, padding="valid")(inputs)
    x = tf.keras.layers.MaxPooling1D(pool_size=2)(x)

    # Segunda capa convolucional y de pooling
    x = tf.keras.layers.Conv1D(filters=64, kernel_size=3, activation="relu", strides=1, padding="valid")(x)
    x = tf.keras.layers.MaxPooling1D(pool_size=2)(x)

    # Tercera capa convolucional y de pooling
    x = tf.keras.layers.Conv1D(filters=128, kernel_size=3, activation="relu", strides=1, padding="valid")(x)
    x = tf.keras.layers.MaxPooling1D(pool_size=2)(x)

    # Aplanar y añadir Dropout
    x = tf.keras.layers.Flatten()(x)
    x = tf.keras.layers.Dropout(0.5)(x)

    # Capa de salida
    outputs = tf.keras.layers.Dense(num_classes, activation="softmax")(x)

    # Crear el modelo usando la API funcional
    model = tf.keras.Model(inputs=inputs, outputs=outputs, name='cnn1d_model')

    return model

def validar_balanceo_dataset(y, class_labels=None, title="Distribución de clases en el dataset"):
    plt.figure(figsize=(8, 5))

    # Obtener el conteo de cada clase
    unique, counts = np.unique(y, return_counts=True)

    # Si hay etiquetas de clase proporcionadas
    if class_labels is not None:
        labels = [class_labels[u] for u in unique]
    else:
        labels = unique

    df = pd.DataFrame({"Clase": labels, "Muestras": counts})

    sns.barplot(data=df, x="Clase", y="Muestras", hue="Clase", palette="tab10", legend=False)
    plt.xlabel("Clase")
    plt.ylabel("Número de muestras")
    plt.title("Distribución de clases en el dataset")
    plt.tight_layout()
    plt.show()

signals = [
    "body_acc_x", "body_acc_y", "body_acc_z",
    "body_gyro_x", "body_gyro_y", "body_gyro_z",
    "total_acc_x", "total_acc_y", "total_acc_z"
]


def load_signals(signal_type="train"):
    data = []
    for signal in signals:
        path = f"sample_data/uci-har/{signal_type}/Inertial Signals/{signal}_{signal_type}.txt"
        signal_data = np.loadtxt(path)  # shape: (n_samples, 128)
        data.append(signal_data)
    return np.stack(data, axis=1)  # (n_samples, 9, 128)

def load_labels(signal_type="train"):
    path = f"sample_data/uci-har/{signal_type}/y_{signal_type}.txt"
    return np.loadtxt(path).astype(int) - 1  # etiquetas de 0 a 5

# carga de datos de entrenamiento
X_train=load_signals("train")
y_train = load_labels("train")
objValidation=validateHumanActivityRecognitionDataSet(X_train)
#validacion de nulos en el dataset
objValidation.validateNulls()
#validacion de blancos en el dataset
objValidation.validateBlanks()
#graficar los datos en dos dimensiones
objValidation.plotInformation(X_train,y_train)

validar_balanceo_dataset(y_train)

"""Vemos en el grafico de distribucion del dataset que a pesar de NO estar fuertemente desbalanceado hay algunas variaciones moderadas en el numero de
muestras entre clases
"""

X_train = load_signals("train")  # (n_samples, 9, 128)
y_train = load_labels("train")   # (n_samples,)
dataset = ConvertirDataSetHAR(X_train, y_train, batch_size=64,validation_split=0.2)
X_val, y_val = dataset.get_validation_data()
model = cnn_1d_model(input_shape=(128, 9), num_classes=6)
model.summary()

#compilamos y entrenamos el modelo
model.compile(
        optimizer=Adam(learning_rate=0.001),
        loss=SparseCategoricalCrossentropy(),
        metrics=['accuracy']
    )

history=  model.fit(
        dataset,
        epochs=30,
        validation_data=(X_val, y_val),
        batch_size=64,
        verbose=1
    )

#Evaluacion del modelo
 # Cargar datos de prueba
X_test = load_signals("test")  # (n_samples, 9, 128)
y_test = load_labels("test")   # (n_samples,)

# Crear generador con la clase personalizada
test_dataset = ConvertirDataSetHAR(X_test, y_test, batch_size=64)

# Evaluar el modelo en el dataset de prueba
results = model.evaluate(test_dataset, verbose=1)

# Realizar predicciones en el conjunto de prueba
X_test_transposed = X_test.transpose(0, 2, 1)
y_pred = model.predict(X_test_transposed)

# Convertir las predicciones en etiquetas (la clase con mayor probabilidad)
y_pred_classes = np.argmax(y_pred, axis=1)
y_true = y_test.astype(int)  # Las etiquetas reales del conjunto de prueba

"""**GRAFICA DE VALORES REALES VS PREDICHOS**"""

import matplotlib.pyplot as plt

def plot_loss_historia_keras(history):
    # Graficar el histórico de pérdida durante el entrenamiento
    plt.plot(history.history['loss'], label='Pérdida de Entrenamiento')
    plt.plot(history.history['val_loss'], label='Pérdida de Validación')
    plt.title('Pérdida durante el Entrenamiento')
    plt.xlabel('Época')
    plt.ylabel('Pérdida')
    plt.legend()
    plt.show()


def plot_acc_historia_keras(history):
    # Graficar la precisión durante el entrenamiento
    plt.plot(history.history['accuracy'], label='Precisión de Entrenamiento')
    plt.plot(history.history['val_accuracy'], label='Precisión de Validación')
    plt.title('Precisión durante el Entrenamiento')
    plt.xlabel('Época')
    plt.ylabel('Precisión')
    plt.legend()
    plt.show()

def plot_matriz_confusion(cm):
    # Visualizar la matriz de confusión usando Seaborn
    plt.figure(figsize=(10, 7))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=range(10), yticklabels=range(10))
    plt.xlabel('Etiqueta predicha')
    plt.ylabel('Etiqueta real')
    plt.title('Matriz de Confusión para el MLP en el dataset MNIST')

def plot_predictions(model):
    X_test = load_signals("test")
    y_test = load_labels("test")
    test_dataset = ConvertirDataSetHAR(X_test, y_test, batch_size=64)

    # Obtener predicciones del modelo
    predictions = model.predict(test_dataset)
    predicted_labels = np.argmax(predictions, axis=1)

    # Graficar las predicciones vs. los valores reales
    plt.figure(figsize=(10, 6))
    plt.plot(y_test, label='Real')
    plt.plot(predicted_labels, label='Predicción')
    plt.xlabel('Muestra')
    plt.ylabel('Etiqueta de Actividad')
    plt.title('Predicciones del Modelo vs. Valores Reales')
    plt.legend()
    plt.grid(True)
    plt.show()

plot_predictions(model)

#graficar Matriz de confunción
cm = confusion_matrix(y_test, y_pred_classes)
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.title('Matriz de Confusión — Validación')
plt.xlabel('Predicho')
plt.ylabel('Real')
plt.show()

print(classification_report(y_true, y_pred_classes))

#graficar perdida durante el entrenamiento
plot_acc_historia_keras(history)

print(f"Precisión final de entrenamiento: {history.history['accuracy'][-1]*100:.2f}%")
print(f"Precisión final de validación: {history.history['val_accuracy'][-1]*100:.2f}%")

plot_loss_historia_keras(history)

print(f"Pérdida final de entrenamiento: {history.history['loss'][-1]*100:.2f}%")
print(f"Pérdida final de validación: {history.history['val_loss'][-1]*100:.2f}%")